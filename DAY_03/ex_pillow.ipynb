{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모듈로딩\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 이미지 데이터 로딩\n",
    "file = \"../data/img/cat.jpg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('JPEG', 225, 225, 'RGB')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catImg = Image.open(file)\n",
    "\n",
    "catImg.format, catImg.height, catImg.width, catImg.mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 225, 225]) 3\n",
      "tensor([[0.8196, 0.8196, 0.8157,  ..., 0.8275, 0.8235, 0.8196],\n",
      "        [0.8196, 0.8196, 0.8157,  ..., 0.8275, 0.8275, 0.8196],\n",
      "        [0.8196, 0.8196, 0.8157,  ..., 0.8275, 0.8275, 0.8157],\n",
      "        ...,\n",
      "        [0.9098, 0.9137, 0.9137,  ..., 0.7765, 0.7765, 0.7725],\n",
      "        [0.9098, 0.9098, 0.9137,  ..., 0.7804, 0.7804, 0.7725],\n",
      "        [0.9137, 0.9137, 0.9137,  ..., 0.7725, 0.7725, 0.7725]])\n"
     ]
    }
   ],
   "source": [
    "### 사용법 => (1) 인스턴스 생성\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "### (2) 인스턴스변수 사용\n",
    "catTS = to_tensor(catImg)\n",
    "\n",
    "### (3) 변환된 이미지 텐서 확인\n",
    "print(catTS.shape, catTS.ndim)\n",
    "print(catTS[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (225, 225, 3) 3\n",
      "<class 'numpy.ndarray'> (225, 225, 3) 3\n",
      "torch.Size([3, 225, 225]) 3 tensor([[0.8510, 0.8510, 0.8471,  ..., 0.8353, 0.8314, 0.8510],\n",
      "        [0.8510, 0.8510, 0.8471,  ..., 0.8353, 0.8353, 0.8510],\n",
      "        [0.8510, 0.8510, 0.8471,  ..., 0.8353, 0.8353, 0.8471],\n",
      "        ...,\n",
      "        [0.9608, 0.9647, 0.9647,  ..., 0.6941, 0.6941, 0.6902],\n",
      "        [0.9608, 0.9608, 0.9647,  ..., 0.6980, 0.6980, 0.6902],\n",
      "        [0.9725, 0.9725, 0.9725,  ..., 0.6980, 0.6980, 0.6980]])\n",
      "torch.Size([3, 225, 225]) 3 tensor([[0.8196, 0.8196, 0.8157,  ..., 0.8275, 0.8235, 0.8196],\n",
      "        [0.8196, 0.8196, 0.8157,  ..., 0.8275, 0.8275, 0.8196],\n",
      "        [0.8196, 0.8196, 0.8157,  ..., 0.8275, 0.8275, 0.8157],\n",
      "        ...,\n",
      "        [0.9098, 0.9137, 0.9137,  ..., 0.7765, 0.7765, 0.7725],\n",
      "        [0.9098, 0.9098, 0.9137,  ..., 0.7804, 0.7804, 0.7725],\n",
      "        [0.9137, 0.9137, 0.9137,  ..., 0.7725, 0.7725, 0.7725]])\n"
     ]
    }
   ],
   "source": [
    "#### openCV\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread(file)  # BGR\n",
    "img2 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # RGB\n",
    "\n",
    "print(type(img), img.shape, img.ndim)\n",
    "print(type(img2), img2.shape, img2.ndim)\n",
    "\n",
    "### 텐서화\n",
    "cat2 = to_tensor(img)\n",
    "cat3 = to_tensor(img2)\n",
    "\n",
    "print(cat2.shape, cat2.ndim, cat2[0])\n",
    "print(cat3.shape, cat3.ndim, cat3[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ==> Resizing\n",
    "preprocessing = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((50, 50)),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8157, 0.8118, 0.8157,  ..., 0.7333, 0.7804, 0.8235],\n",
       "         [0.8157, 0.8118, 0.8157,  ..., 0.7333, 0.7843, 0.8235],\n",
       "         [0.8157, 0.8118, 0.8157,  ..., 0.7373, 0.7922, 0.8157],\n",
       "         ...,\n",
       "         [0.9176, 0.9216, 0.9216,  ..., 0.7686, 0.7608, 0.7608],\n",
       "         [0.9176, 0.9255, 0.9216,  ..., 0.7804, 0.7686, 0.7686],\n",
       "         [0.9137, 0.9176, 0.9098,  ..., 0.7804, 0.7765, 0.7765]],\n",
       "\n",
       "        [[0.8039, 0.8000, 0.8039,  ..., 0.6941, 0.7529, 0.8039],\n",
       "         [0.8039, 0.8000, 0.8039,  ..., 0.6980, 0.7608, 0.8039],\n",
       "         [0.8039, 0.8000, 0.8039,  ..., 0.7020, 0.7686, 0.8000],\n",
       "         ...,\n",
       "         [0.9451, 0.9490, 0.9490,  ..., 0.6431, 0.6353, 0.6353],\n",
       "         [0.9451, 0.9529, 0.9490,  ..., 0.6549, 0.6431, 0.6431],\n",
       "         [0.9412, 0.9451, 0.9373,  ..., 0.6549, 0.6510, 0.6510]],\n",
       "\n",
       "        [[0.8471, 0.8431, 0.8471,  ..., 0.7137, 0.7765, 0.8314],\n",
       "         [0.8471, 0.8431, 0.8471,  ..., 0.7137, 0.7843, 0.8353],\n",
       "         [0.8471, 0.8431, 0.8471,  ..., 0.7176, 0.7922, 0.8275],\n",
       "         ...,\n",
       "         [0.9686, 0.9725, 0.9725,  ..., 0.6863, 0.6784, 0.6784],\n",
       "         [0.9686, 0.9765, 0.9725,  ..., 0.6980, 0.6863, 0.6863],\n",
       "         [0.9686, 0.9686, 0.9608,  ..., 0.6980, 0.6941, 0.6941]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(catImg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 데이터 셋 <hr>\n",
    "\n",
    "-   torchvision.ImageFolder 클래스 사용\n",
    "    -   이미지 데이터 라벨링\n",
    "    -   이미지 데이터 전처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = \"../data/img\"\n",
    "\n",
    "imgDS = ImageFolder(root=img_root, transform=preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cat', 'dysonsphere', 'flower'],\n",
       " {'cat': 0, 'dysonsphere': 1, 'flower': 2},\n",
       " [('../data/img\\\\cat\\\\cat.jpg', 0),\n",
       "  ('../data/img\\\\cat\\\\cat01.jpg', 0),\n",
       "  ('../data/img\\\\cat\\\\cat02.jpg', 0),\n",
       "  ('../data/img\\\\cat\\\\cat03.jpg', 0),\n",
       "  ('../data/img\\\\cat\\\\cat04.jpg', 0),\n",
       "  ('../data/img\\\\cat\\\\cat05.jpg', 0),\n",
       "  ('../data/img\\\\cat\\\\cat05_copy.jpg', 0),\n",
       "  ('../data/img\\\\dysonsphere\\\\DysonSphere.jpg', 1),\n",
       "  ('../data/img\\\\dysonsphere\\\\DysonSphere01.jpg', 1),\n",
       "  ('../data/img\\\\dysonsphere\\\\DysonSphere02.jpg', 1),\n",
       "  ('../data/img\\\\dysonsphere\\\\DysonSphere03.jpg', 1),\n",
       "  ('../data/img\\\\dysonsphere\\\\DysonSphere04.jpg', 1),\n",
       "  ('../data/img\\\\dysonsphere\\\\DysonSphere05.jpg', 1),\n",
       "  ('../data/img\\\\flower\\\\flower01.jpg', 2),\n",
       "  ('../data/img\\\\flower\\\\flower02.jpg', 2),\n",
       "  ('../data/img\\\\flower\\\\flower03.jpg', 2)])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 폴더명이 분류 클래스\n",
    "imgDS.classes, imgDS.class_to_idx, imgDS.imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n",
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n",
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n",
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n",
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n",
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n",
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n",
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n",
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n",
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n",
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n",
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n",
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n",
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n",
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n",
      "torch.Size([1, 3, 50, 50]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "imgDL = DataLoader(imgDS)\n",
    "\n",
    "for img, label in imgDL:\n",
    "    print(img.shape, label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
